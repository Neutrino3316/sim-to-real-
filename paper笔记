2018.5.16 Google Brain,X,Google DeepMind

Abstract:
In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques (DRL 使机器人自动行驶)

Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. （简单的奖励信号，开环， ？scratch ）

We narrow this reality gap by improving the physics simulator and learning robust policies（重点）

*****We improve the simulation using system identiﬁcation, developing an accurate actuator model and simulating latency（？系统辨识， 模拟潜在因素）

 *****We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space（物理环境随机，增加扰动，？设计紧凑的观察空间）

1.Introduction
困难：control an under-actuated robot performing highly dynamic motion that involve intricate balance.（?欠驱动）
 


 仿真困难：This reality gap [7, 8] is caused by model discrepancies between the simulated and the real physical system. Many factors, including unmodeled dynamics, wrong simulation parameters, and numerical errors, contribute to this gap.（现实世界和仿真环境的差别，因素有未建模的动力学，错误的仿真参数和数值错误）
Even worse, this gap is greatly ampliﬁed in locomotion tasks.（错误扩增）
 When a robot performs agile motion with frequent contact changes, the switches of contact situations break the control space into fragmented pieces. （？controll space）

现实学习困难： due to the difﬁculties of automatically resetting the experiments and continuously collecting data

此peper的两大挑战： 1) learning controllable locomotion policies; and 2) transferring the policies to the physical system. 

2.Related Work
A.Legged Locomotion Control
PPO：In this paper, we choose to use Proximal Policy Optimization (PPO) [5] because it is a stable on-policy method and can be easily parallelized 
B.Overcoming the Reality Gap

3.Robot Platform And Physcics Simulation
介绍机器人：Our robot platform is the Minitaur from Ghost Robotics (Figure 1 bottom), a quadruped robot with eight direct-drive actuators [50]. Each leg is controlled by two actuators that allow it to move in the sagittal plane. （8个执行器）

PyBullet（机器人建模的python模块）

4.Learning Locomotion Controllers
A.Background
We formulate locomotion control as a Partially Observable MarkovDecisionProcess(POMDP)and solve it usinga policy gradient method （POMDP, 价值策略迭代）

Our problem is partially observable because certain states such as the position of the Minitaur’s base and the foot contact forces are not accessible due to lack of corresponding sensors.（解释部分可观察的原因，）

Reinforcement learning optimizes a policy π : O 7→ A that maximizes the expected return (accumulated rewards)
目标） 

B.Observation and Action Space 
In our problem, the observations include the roll, pitch, and the angular velocities of the base along these two axes, and the eight motor angles
（?roll, ?pitch, ?the angular velocities of the base along these two axes, and the eight motor angles)

 More importantly, a compact observation space helps to transfer the policy to the real robot. （解释）

 The actions consist of the desired pose of each leg in the leg space.The pose of each leg is decomposed into the swing and the extension components (s,e) (Figure 3). (？leg space， ？motor space)

C.Reward Fucntion
r = (pn −pn−1)·d−w∆t|τn · ˙ qn| ：Since the learning algorithm is robust to a wide range of w, we do not tune it and use w = 0.008 in all our experiments（w是可以调的）

During training, the rewards are accumulated at each episode. An episode terminates after 1000 steps or when the simulated Minitaur loses balance: its base tilts more than 0.5 radians. （无模型的抽样，抽样要求）


D.Policy Representation
 For this reason, we decouple the locomotion controller into two parts, an open loop component that allows a user to provide reference trajectories and a feedback component that adjusts the leg poses on top of the reference based on the observations.
( ?reference trajectories， ？feedback component)
( locomotion controller，) ==》 用户可以表达想要的gait

This hybrid policy (eq. (3)) is a general formulation that gives users a full spectrum of controllability. （提供了用户可以控制的范围）


If we want to use a user-speciﬁed policy, we can set both the lower and the upper bounds of π(o) to be zero. If we want a policy that is learned from scratch, we can set ¯ a(t) = 0 and give the feedback component π(o) a wide output range. （选择policy）

We represent the feedback component π with a neural network and solve the above POMDP using Proximal Policy Optimization [5]. The neural network has two fully-connected hidden layers. Its size is determined via hyperparameter search. Refer to Section VI for more details

5.Narrow The Reality Gap
A.Improving Simulation Fidelity
We disassemble a Minitaur1, measure the dimension, weigh the mass, ﬁnd the center of mass of each link and incorporate this information into the URDF ﬁle。 Measuring inertia is difﬁcult. Instead, we estimate it for each link given its shape and mass, assuming uniform density.We also design experiments to measure motor frictions
（机器人的仿真）
In addition to system identiﬁcation, we augment the simulator with a more faithful actuator model and latency handling. （？actuator model）

 We use position control to actuate the motors of the Minitaur（？position control）

